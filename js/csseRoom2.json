{
    "csse": [
        {
            "time": "12:30 PM - 12:45 PM",
            "projectId": "csse-2-1230",
            "title": "Automating Scripts & Monitoring Applications",
            "studentName": "Ani Avetian",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - T-Mobile",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Ani Avetian.png",
            "abstract": "Over the summer the internship was conducting with a Technical Operations Team at T-Mobile. The aim of the capstone was to help the team achieve the goal of making sure that T-Mobile applications were running and available 99.6% of the time.\n\nTo contribute to the goal, different items needed to be done. The first was writing scripts. These scripts helped the team comply with Sarbanes-Oxley, compliance rules to make sure the correct people had access to the applications. Here the performance of the script was important as it needed to be automated to run over 300 servers to find the people who had access.\n\nThe second involved being a project manager during a knowledge transfer. During this time, the team needed to learn new technologies related to keeping the T-Mobile applications running and available at the specific threshold. As a project manager, good communication, leadership, and organization all came into play to drive the team to success in learning these new technologies. A SharePoint and Confluence site were created to keep track of the new things being learned by the team and meetings were set up regularly to gather more information. \n\nThrough these experiences, the goal of making sure T-Mobile applications were running was met, not just by using technical skills, like writing scripts, but also soft skills, like teamwork and leadership. After leaving the team, they were able to carry off where things were left because of the organizational structure the team worked under. "        
        },
        {
            "time": "12:45 PM - 1:00 PM",
            "projectId": "csse-2-1245",
            "title": "Salesforce Business Technology Internship: Permissions Manager",
            "studentName": "Fatima Diallo",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - Salesforce",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Fatima Diallo.png",
            "abstract": "During my software engineer internship at Salesforce, I worked on Permissions Manager, a tool to help Salesforce administrators add user access roles and field-level permissions to their organizations efficiently. I worked in an agile team with the guidance of my manager and a senior engineer and alongside other interns to develop this application.\n\nSalesforce creates and supports customer relationship management (CRM) software that helps break down the technology silos between departments to give companies a complete view of their customer everywhere they interact with the companies’ brand. With Salesforce orgs, admins can select which roles have access to select information. Assigning those permissions to the necessary profiles for both new and existing objects/fields in the traditional manual method can be quite time-consuming and prone to error. This is especially true for large organizations with hundreds of profiles and new objects/fields being added constantly. With the Permissions Manager app, any admin can deploy many permissions with just a couple clicks.\n\nWe built Permissions Manager in APEX, similar to Java, and LWC which is a packaged form of JavaScript, HTML, and CSS. I worked on the displaying the uploaded csv file of desired permissions, submission of those permissions for processing, and validation of requested permissions. This second version is able to accept upload of csv files for batch imports, display errors for invalid permission entries, and allows applying permissions for multiple profiles in one entry. It will be released on Salesforce AppExchange."
        },
        {
            "time": "1:00 PM - 1:15 PM",
            "projectId": "csse-2-100",
            "title": "Mobility Service Change Intake and Automation",
            "studentName": "Fatima Farah",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - AT&T",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Fatima Farah.png",
            "abstract": "The main aim of this capstone project was to develop a prototype to collect and filter Mobility Service change requests per special criteria. This project would improve business efficiency within the org by having the change requests of multiple packet core gateways available to front end users. \n\nThe first step in building the prototype was to first understand what a zero rating config looks like in NeoConsumer, FirstNet, and Blackbird gateway. This required looking at the config files of select gateways and studying their structure, where specific parts of the rating was located, and learning the service rule hierarchy. \n\nOnce the appropriate zero rating structure was understood, the next step was to design and code automation scripts to be able to parse production gateway config files and output the contained zero rating rules. Testing was performed both with different config files (which are produced daily) and manually fed them to the script. \n\nNext part of the project was coding another automation script that augments the resulting file from the previously made scripts of service rules with rules usage information from the gateway's corresponding usage report. \n\nFor all scripts, performance accuracy and time optimization were at odds with each other, so designing code that balances both was important. \n\nThe approach found that providing accurate results without sacrificing too much performance was a combination of understanding the config files plus zero rating structure thoroughly and not coding for static values.  The end result was faster analysis of Mobility Service change requests by hours. This was successfully built, tested with Python, Eclipse, and Windows command line. The results were satisfactory, if slower then desired."        
        },
        {
            "time": "1:15 PM - 1:30 PM",
            "projectId": "csse-2-115",
            "title": "Mobile Application for Sanford Irrigation",
            "studentName": "Ethan Gordash",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - Sanford Irrigation",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Ethan Alexander Gordash.jpeg",
            "abstract": "The mobile application that I built for Sanford Irrigation serves as a customer database to help improve efficiency for employees doing fieldwork. Specifically, the app allows employees to search for a customer and find information about the customer’s irrigation system. Different aspects of the irrigation system that the employees can research are the customer’s timer, shut-off valves, electric solenoid valves, and miscellaneous parts of the system. \n\nThe first step to build the mobile application was to research an efficient way in which store the data in the cloud that could be accessed on both Android and iOS devices. The answer to this was to use Google Cloud Firestore for cloud storage and React Native to program the app. The reasoning for this was due to Firestore being highly secure and easy to use and React Native allowing me to use one code base for both Android and iOS devices. \n\nFollowing the initial research phase, the next step was building the app and performing careful testing in each phase of development. In terms of the flow of the project, the backend was built and tested first followed by the front-end. \n\nNow that the app is complete, employees of Sanford Irrigation are using the app and populating the customer database. Given that they had been searching for a while for a way to create a database with information on customer’s irrigation systems, they are quite pleased with the finished product as it will help their employees be more efficient while performing field work."        
        },
        {
            "time": "1:30 PM - 1:45 PM",
            "projectId": "csse-2-130",
            "title": "Internship with Sirrus7 Creating Logic Platform Product",
            "studentName": "Evan Jensen",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - Sirrus 7",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Evan Kyle Jensen.png",
            "abstract": "The main aim of this capstone project was to develop a minimum viable product (MVP) of the Logic Platform through an internship with sirrus7. Sirrus7 is a software contracting company and the Logic Platform was developed as a startup within sirrus7. \n\nClients often hire software contractors to build automation rules within their software environment to automate tasks. As time goes on, new automation rules are required, and clients must re-hire software contractors to build these new automation rules. The Logic Platform’s mission is to offer a solution in which the client can create, update, and delete automation rules without having to re-hire software contractors.\n\nThe objective of the MVP for the Logic Platform was to design and implement it in a modularized and scalable fashion for optimized connectivity with other software environments and to add an integrated module of Gitlab, an open-source version control system. This objective was met by utilizing React for front end development, Golang for backend development, and PostgreSQL for database management. Gitlab’s software environment was used as a test client. Rules were implemented in the Logic Platform’s Gitlab integration to automate data manipulation tasks within the Gitlab environment using HTTP for communication between applications.\n\nOnce the Logic Platform is connected to the client’s software environment, the client can easily create automation rules on their own using the Logic Platform UI. This eliminates the need to continually re-hire software contractors to build out new automation features in the client’s software environment. Due to the modularization and decoupled implementation of the Logic Platform, it has massive potential to disrupt how contractual software services are currently conducted in the industry."        
        },
        {
            "time": "1:45 PM - 2:00 PM",
            "projectId": "csse-2-145",
            "title": "Anomaly Detection Service",
            "studentName": "Tae Kwon",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - Amazon",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Tae Young Kwon.jpeg",
            "abstract": "My internship team is part of the AWS that supports Supply Chain Organization mainly works on Auth-N & Auth-Z. The team is responsible for checking authentication and authorization of the users when they login. Currently, once the user is logged in, there are no data that determines user’s anomalous behavior. The internship project is to build anomalous detection service to monitor user, once they login.  \n\nThe process of the internship required many back and forth of meetings to clear ambiguities, and to adjust the scope of the project that is accomplishable within the internship time. For instance, the term anomalous(anomaly) is a such a wide topic that covers huge aspect of the cyber security. The first thing that I had to work on was to define definition of the anomaly, and what anomalous behavior that will be covered. Over the time after the research and POC, we found the technical limitations that the scope changed to analyze users’ login pattern to catch rudimentary anomalous behavior. \n\nThe service is built with Java and AWS proprietary framework. It utilizes multiple native AWS services – Cognito, CloudTrail, S3, Lambda, and DynamoDB. Over the internship, the micro service is accomplished and the team is satisfied to see the possible use of CloudTrail. "        
        },
        {
            "time": "2:00 PM - 2:15 PM",
            "projectId": "csse-2-200",
            "title": "Creating IoT Beyond-The-Smartphone Sensors",
            "studentName": "Kevin Xu",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - T-Mobile",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Kevin Xu.png",
            "abstract": "T-Mobile is a telecommunication and mobile service company headquartered in Bellevue, Washington. One of the key places that T-Mobile is expanding development in is products beyond the smartphone. To enter this market, T-Mobile began to release their own versions of various products that directly compete with their current competitors. This is especially useful so that these products can later be synced with T-Mobile smartphones and other T-Mobile branded products. With new products coming down the pipe, many need various sensors to detect different items, such as temperature.\n\nAs a software intern, my intern project was to design, develop and create a temperature sensor driver for a temperature sensor chip on our T-Mobile Starter Board Development Kit to create mock-up data that can be later used for future references and products. This was part of a collaboration between all the interns on my team for the greater intern project to produce a sample development kit and app for future development kits. Through agile methods and sprint planning, steps were laid out on how to approach this situation. My process to create this this sensor was to first research the new open-source operating system that the sensor was going to be built upon, developing code based on other temperature sensor code, creating unit tests to test against the sensor code, and displaying the code to a client as the main checkpoints. \n\nThe result was that the temperature sensor prototype was created and was able to take in real time temperature data, and for future work, would be worked into an actual temperature sensor device."        
        },
        {
            "time": "2:15 PM - 2:30 PM",
            "projectId": "csse-2-215",
            "title": "Franc the Friendly Slack Bot",
            "studentName": "Jibran Ahmed",
            "studentMajor": "CSSE",
            "projectType": "Sponsored Internship - T-Mobile",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/Jibran Ahmed.jpg",
            "abstract": "The purpose of this project was to automate functionalities and create a new feature within the Slack bot, known as Franc. Franc is a bot that utilizes Natural-Language-Processing (NLP) and AI/ML to improve customer satisfaction and increase team productivity. Functionalities that were automated are the test automation scripts, the deployment of code to GitLab. The feature that was created was a code-deployment scheduler.\n\nThe first step in automating the test automation scripts was to understand how the current feature works with Slack bots, as well as the Slack API. This was done through bootcamp projects working with REST APIs and incorporating them into Slack. Understanding the foundation and format of the bot was crucial before implementing any key features.\n\nAfter completing the bootcamps, understanding Franc code and debugging, the next step was to start automating the existing test automation feature. The test automation scripts would be run through Slack commands, but the command may not work if you misspelled an API name or missed a test environment. The user would also have to know beforehand what API they want to use. Using MongoDB, the list of APIs would automatically be retrieved from the database so that the user didn’t have to type anything. This would be the same process for the testing environment, the testing type, and the foundation of the environment. \n\nThe next step was implementing the scheduler to deploy code. While scheduled, it will interact with the Cassandra database and update the JIRA board automatically. The JIRA board will update and display the pipeline stages, the deployment status, and the bot that deployed the code.\n\nThe results increased efficiency of productivity. Incorporating the automation feature to the testing allowed the time efficiency to improve by decreasing it from 30 minutes to 20 seconds. When running test scripts previously, the user would not be able to do other tasks. Now the user can run multiple tests while being able to do other things. Through automation, the user does not have to download, build, or compile the code, rather just do it through Slack through their laptops or smartphones. To deploy code, members of the team would have to stay up until 8:00 P.M. to deploy code, but now the team can schedule the deployment and log off. This project resulted in saving the company hundreds of thousands of dollars in terms of labor investment."        
        },
        {
            "time": "2:30 PM - 2:45 PM",
            "projectId": "csse-2-230",
            "title": "Workbench: A Provenance Visualization Tool",
            "studentName": "Shakeel Khan",
            "studentMajor": "CSSE",
            "projectType": "UWB CSS Faculty Research",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/Shakeel Khan.png",
            "abstract": "For my capstone I worked on the Graphitti Workbench with Professor Stiber. Graphitti Workbench (or Workbench) is a graphically based provenance visualization tool designed to work with the Graphitti Simulator, a parallel simulation framework for graph-based systems, designed to enable researchers in need of a high-performance solution to develop large-scale and long-duration simulations. Graphitti is currently being applied to computational neuroscience and emergency communications systems.\n\nWorkbench allows users to create and run new Graphitti simulations, analyze their output, compare it with the results from other experiments, and derive new experiments from existing ones. It keeps track of all the artifacts related to an experiment such as the version of the simulator used, the configuration files, and all other input/output files. This is where the provenance comes in. It’s the ability to record the history of data – where it came from, the artifacts related to it and the processes that influenced it. This speeds up the scientific workflow tremendously and helps improve experiment replicability.\n\nWhen I started my capstone, Workbench only worked with Graphitti’s predecessor, BrainGrid (which was specifically designed for computational neuroscience). One of my contributions was updating Workbench to work with the Graphitti simulator. I did this by examining the process by which Workbench creates and runs a simulation, looking through the codebase and taking notes to see what classes were involved and how they interacted with one another. From there I was able to identify what changes I needed to make while reducing the chance of breaking anything.\n\nI also contributed to the internal developer documentation. There were several deeply rooted issues I investigated but didn’t have the time to solve myself, so I wrote up in-depth explanations of the issues explaining the current behavior and its causes, along with the expected behavior and possible solutions to achieve it. I also documented some programmer workflows for future contributors to help in keeping Workbench up to date with Graphitti.\n\nAs a result, Workbench has now been updated to work exclusively with the new Graphitti simulator, getting it closer to becoming production ready. As for the documentation, my actions will help future contributors working on similar parts of the project and contributes to one of the goals of better documenting the internals of Workbench."        
        },
        {
            "time": "2:45 PM - 3:00 PM",
            "projectId": "csse-2-245",
            "title": "Graphitti - Improving Software Maintainability",
            "studentName": "James Kim",
            "studentMajor": "CSSE",
            "projectType": "UWB CSS Faculty Research",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/James Kim.png",
            "abstract": "Graphitti is an open-source high-performance graph-based event simulator being developed by the Intelligent Networks Laboratory at UW Bothell. It provides scientists and researchers pre-built code that can be modified to meet their requirements. It can run on either a CPU or GPU, but a GPU can provide at least twenty times the performance over a CPU by utilizing multi-threading.\n\nGraphitti was built by adapting subsystems from its predecessor, BrainGrid, which was developed specifically for graph-based neural network simulations. The development process of Graphitti involved abstracting the graph-based functionality from the neural-specific design of BrainGrid. This required implementing new design patterns throughout the software so that it could be a general-purpose simulator that is adaptable to other fields. \n\nMy work on the project was mostly related to the recorder subsystem. A recorder periodically copies data from other subsystems during a simulation and writes the data to an output file. \n\nDuring the beginning of my capstone, I was tasked with making small changes within the recorders which helped to get me up to speed. I learned how to set up Visual Studio Code for remote development and use the integrated GitHub features inside of Visual Studio Code. I refreshed myself on design patterns before taking on bigger tasks.\n\nA major issue with the current architecture is that if a user wanted to use Graphitti for a new type of simulation, the user would have to implement a new recorder from scratch. This takes away from the maintainability and usability of Graphitti. For me to come up with a viable solution, I had to familiarize myself with the recorder system and the other systems that it interacts with. This was a time-consuming process due to the large codebase. Throughout this process I documented how the recorders work, and with the help of Dr. Stiber, was able to plan a solution and document that as well.\n\nMy contributions to the project will help future team members work on the recorder system and will help the usability for users of the software.\n\nThis capstone has given me valuable experience working on a large project with other team members. It has also taught me the importance of software design principles and the need for good documentation."        
        },
        {
            "time": "3:00 PM - 3:15 PM",
            "projectId": "csse-2-300",
            "title": "Graphitti: A Graph-Based Simulator",
            "studentName": "Mark Sorvik",
            "studentMajor": "CSSE",
            "projectType": "UWB CSS Faculty Research",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/Mark Anthony Sorvik.jpg",
            "abstract": "Graphitti is an open-source project produced by the Intelligent Networks Laboratory. It consists of a graph-based systems simulator that allows CPU powered and GPU/CUDA accelerated simulations, as well as an accompanying tool called Workbench that manages data and software provenance for these simulations.  \n\nThe objective of the Graphitti project is to facilitate the creation of simulations in such a way that makes the conversion to GPU accelerated versions easier and more reliable, and to provide the tools so that upkeep, updates, and analysis of these simulations and their data is easier and more reliable. \n\nThe Graphitti graph-based simulator is based on the BrainGrid neural simulator, a separate project previously produced by the Intelligent Networks Laboratory. Graphitti was made to handle any type of graph-based simulation, instead of just neural network simulations like its predecessor. Portions of BrainGrid specific to neural network simulation were abstracted out of Graphitti at the top level and separated into a neural simulation model implemented using Graphitti's graph-based simulator capabilities. NG911, or Next Gen 911, is another type of simulation model currently implemented using Graphitti's graph-based simulator capabilities, which focuses on simulating emergency communications systems. \n\nThe transition from BrainGrid to Graphitti was and still is ongoing in some ways. When I started my work on the project the GPU/CUDA accelerated portion of the Graphitti simulator was not fully functional and had several undiscovered bugs. I dedicated most of my effort to getting this GPU/CUDA accelerated portion of Graphitti working correctly and bringing it up to parity with the GPU accelerated portion of its predecessor, BrainGrid, that it replaced. As of the completion of my capstone the GPU accelerated portion of Graphitti is functional. \n\nI also completed other tasks for the lab such as standardizing and expanding the set of simulations used to test and validate the simulator, helping update GitHub regression testing, restructuring and partially rewriting the CMake implementation, adding and updating documentation, solving other bugs, reviewing and approving pull requests, and generally helping other lab members. Over the course of this capstone I gained valuable experience on integrating into a software project with a large existing code base, collaborating with a large team, reading and interpreting others' code, debugging complex systems, practicing good version control, and writing documentation, among other things."        
        },
        {
            "time": "3:15 PM - 3:30 PM",
            "projectId": "csse-2-315",
            "title": "Developing Education Software on Web-Based Neural Networks",
            "studentName": "Angelina Tang",
            "studentMajor": "CSSE",
            "projectType": "UWB CSS Faculty Research",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/Angelina Tang.PNG",
            "abstract": "For the average college freshman, topics such as machine learning, artificial intelligence, and neural networks are interesting but seem inaccessible. With the next wave of technology being centered around understanding the human mind and curating innovations to streamline interaction with products and eliminate human error, the current generation needs to be educated in current technological endeavors. Dr. Michael Stiber, teamed with Dr. Madeleine Gorges, aims to combat the impression of the necessity of a strong programming background and help students interested in computers and cognition create a connection between the two sciences.\n\nFor my capstone, I worked with Dr. Stiber to develop educational software on web-based neural networks for a freshman (Discovery Core) class titled Computers and Cognition that was offered during the 2021 Autumn Quarter. My task was to find open-source JavaScript web applications, modify them to fit the lessons that Dr. Stiber wanted to teach, and host them on the University of Washington servers for instructional use.\n\nThe applications I created were based off of CNN (Convolutional Neural Networks) Explainer. Two versions of the application were created in order to show the effects of bias in a dataset; the hope was that this application could be used to teach students about the importance of unbiased data and the consequences of biased data being used. I worked with JavaScript and Python to modify training, testing, and validations sets and created bash scripts to streamline the process of webhosting them."        
        }
    ]
}